"use strict";(self.webpackChunkpjay_in=self.webpackChunkpjay_in||[]).push([[807],{7946:function(e,t,n){n.d(t,{Z:function(){return s}});var a=n(6540),r=n(336),l=n(9581);var s=e=>{let{heading:t,headingClass:n="",bodyClass:s="",sideComponent:i,sideImg:o,sideClass:c="",className:m="",sideImgClassName:u="",id:d="",children:h}=e;return a.createElement("div",{className:m,id:d},a.createElement("div",{className:`${l.A.page.xl} grid-12 gutter-row-20 gutter-36-ns`},a.createElement("div",{className:`${s}`},a.createElement("h1",{className:`${n}`},t),h),o||i?a.createElement("div",{className:`${c}`},o?a.createElement(r.A,{name:o,className:u}):i):null))}},8858:function(e,t,n){n.r(t);var a=n(6540),r=n(1515),l=n(2269),s=n(7946);t.default=e=>{const{0:t,1:n}=(0,a.useState)(!1);return a.createElement(r.A,{headerClass:""},a.createElement(l.A,{title:"A tiny HRM 27M beat Opus 4 and o3 on the ARC AGI benchmark",description:"That sounds very surprising. Most of the SOTA models we hear about are hundreds of billions to a few trillion parameters. HRM shows something different can work for reasoning on limited data."}),a.createElement("div",{class:"main-content",style:{minHeight:"100vh"}},a.createElement(s.Z,{heading:"A tiny HRM 27M beat Opus 4 and o3 on the ARC AGI benchmark",headingClass:"ma0 pa0 f2 f-headline-ns sig-blue fw-600",bodyClass:"col-12 mw-100 center",className:"pt16"}),a.createElement("div",{className:" pt0 pb5 pt10-ns pb20-ns"},a.createElement("div",{className:"mw-l center"},a.createElement("p",{className:"ma0 pa0 pl5 pr5 mt4 f4 f3-ns sig-grey"},"That sounds very surprising.",a.createElement("br",null),a.createElement("br",null),"Most of the SOTA models we hear about (Claude, GPT, Gemini, Grok) are hundreds of billions to a few trillion parameters. We can't realistically run those locally. So when Sapient Intelligence published HRM (paper in June, model out in July) everyone was impressed that something a thousand times smaller can outperform larger counterparts on a ARC AGI benchmark. But size alone doesn't tell the full story.",a.createElement("br",null),a.createElement("br",null),"HRM scored 32% on ARC AGI 1 and 2% on ARC AGI 2 without pre-training. That's the interesting part, as the current foundation models are trained on almost the entire internet. It shows something different can work for reasoning on limited data.",a.createElement("br",null),a.createElement("br",null),"For context from recent large models:",a.createElement("br",null),"- Qwen3-coder (480B parameter model) was trained on 7.5 trillion tokens",a.createElement("br",null),"- Kimi-k2 (1T parameter model) saw 15.5 trillion tokens",a.createElement("br",null),a.createElement("br",null),"As we understand HRM is not a foundation model. It lacks:",a.createElement("br",null),"1. Broad pre-training data.",a.createElement("br",null),"2. Transfer learning capability.",a.createElement("br",null),a.createElement("br",null),"So we can't expect HRM to write polished emails or build software apps from scratch. What it can do is solve logical puzzles and show reasoning aptitude exactly what ARC AGI measures.",a.createElement("br",null),a.createElement("br",null),a.createElement("strong",null,"How does HRM work?"),a.createElement("br",null),a.createElement("br",null),"HRM's core novelty is architectural, inspired (loosely) by neuroscience. Instead of one transformer predicting the next token, HRM uses a dual recurrent loop. Two recurrent neural networks operating at different time scales:",a.createElement("br",null),a.createElement("br",null),"- A fast, lower-level module that generates bursts of thinking (rapid, local processing)",a.createElement("br",null),"- A slower, higher-level module that is more abstract and refines the fast module's outputs",a.createElement("br",null),a.createElement("br",null),"Think of it as fast intuition + slow deliberation. The higher module provides guidance, the lower module produces quick hypotheses which are iteratively refined. This outer-refinement loop feeding fast bursts into a slower, abstract controller is what gives HRM its compositional reasoning advantage on specific tasks.",a.createElement("br",null),a.createElement("br",null),"Transformers, by contrast, optimize next-token prediction over large contexts and scale extremely well in parallel training. RNNs historically struggled with long-term dependencies and early convergence to suboptimal representations. HRM sidesteps some of those issues by bifurcating timescales and adding that iterative refinement.",a.createElement("br",null),a.createElement("br",null),a.createElement("br",null),a.createElement("strong",null,"Why this matters?"),a.createElement("br",null),a.createElement("br",null),"HRM is not a transformer killer or an immediate replacement for foundation models. It's not a threat to the scale and pre-training paradigm that powers GPT-class models. Instead, it's a refinement in the AGI/reasoning direction an architecture that shows RNN-style systems can be competitive on reasoning tasks without trillions of tokens.",a.createElement("br",null),a.createElement("br",null),"Now RNNs could come back. We've traded a lot of learning quality for speed of scaling. If new architectures learn better, we might not need monstrous models to get strong reasoning.",a.createElement("br",null),a.createElement("br",null),"A path toward more efficient AGI-style reasoning. Transformer scale has limits eventual saturation, huge compute, and the impossibility of local runs. HRM hints at designs that might be more compute-friendly for certain classes of reasoning.",a.createElement("br",null),a.createElement("br",null),"Biology-inspired loops are promising again. The outer refinement loop is simple but powerful iterate, correct, repeat. That's central to human reasoning and now seems useful in neural nets too.",a.createElement("br",null),a.createElement("br",null),"Pre-training is still a massive advantage. Transformers win because they parallelize well and learn from massive batches. HRM hasn't been scaled with that same pre-training playbook yet.",a.createElement("br",null),a.createElement("br",null))))))}}}]);
//# sourceMappingURL=component---src-pages-writings-hrm-24-js-b13a65c0362b50c85af2.js.map