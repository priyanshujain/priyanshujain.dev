{"version":3,"file":"component---src-pages-writings-hrm-24-js-b13a65c0362b50c85af2.js","mappings":"wKAuCA,MAnCmBA,IAWZ,IAXa,QAClBC,EAAO,aACPC,EAAa,GAAE,UACfC,EAAU,GAAE,cACZC,EAAa,QACbC,EAAO,UACPC,EAAU,GAAE,UACZC,EAAU,GAAE,iBACZC,EAAiB,GAAE,GACnBC,EAAG,GAAE,SACLC,GACDV,EACC,OACEW,EAAAA,cAAA,OAAKJ,UAAWA,EAAWE,GAAIA,GAC7BE,EAAAA,cAAA,OAAKJ,UAAW,GAAGK,EAAAA,EAAUC,KAAKC,yCAChCH,EAAAA,cAAA,OAAKJ,UAAW,GAAGJ,KACjBQ,EAAAA,cAAA,MAAIJ,UAAW,GAAGL,KACfD,GAEFS,GAEFL,GAAWD,EACVO,EAAAA,cAAA,OAAKJ,UAAW,GAAGD,KAChBD,EACCM,EAAAA,cAACI,EAAAA,EAAI,CAACC,KAAMX,EAASE,UAAWC,IAEhCJ,GAGH,MAED,C,0EC8EV,UApGca,IACZ,MAAM,EAACC,EAAc,EAACC,IAAcC,EAAAA,EAAAA,WAAS,GAQ7C,OACET,EAAAA,cAACU,EAAAA,EAAM,CAACC,YAAY,IAClBX,EAAAA,cAACY,EAAAA,EAAG,CACFC,MAAM,6DACNC,YAAa,oMAEfd,EAAAA,cAAA,OACEe,MAAM,eACNC,MAAO,CACLC,UAAW,UAGbjB,EAAAA,cAACkB,EAAAA,EAAU,CACT5B,QAAQ,6DACRC,aAAa,2CACbC,UAAU,uBACVI,UAAU,SAEZI,EAAAA,cAAA,OAAKJ,UAAU,4BACbI,EAAAA,cAAA,OAAKJ,UAAU,eACbI,EAAAA,cAAA,KAAGJ,UAAU,yCAAwC,+BAEnDI,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,yZAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,sPAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,wCAENA,EAAAA,cAAA,WAAM,0EAENA,EAAAA,cAAA,WAAM,0DAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,4DAENA,EAAAA,cAAA,WAAM,8BAENA,EAAAA,cAAA,WAAM,mCAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,0LAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WACAA,EAAAA,cAAA,cAAQ,sBACRA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,mOAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,2FAENA,EAAAA,cAAA,WAAM,8FAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,iUAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,+UAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WACAA,EAAAA,cAAA,cAAQ,qBACRA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,sVAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,wLAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,oPAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,oMAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,WAAM,6LAENA,EAAAA,cAAA,WACAA,EAAAA,cAAA,eAKD,C","sources":["webpack://pjay.in/./src/components/home/SectionBox.js","webpack://pjay.in/./src/pages/writings/hrm24.js"],"sourcesContent":["import React from \"react\";\nimport Icon from \"../Icon\";\nimport styleVars from \"../../styles/style-vars\";\n\nconst SectionBox = ({\n  heading, \n  headingClass=\"\", \n  bodyClass=\"\", \n  sideComponent,\n  sideImg, \n  sideClass=\"\", \n  className=\"\",\n  sideImgClassName=\"\",\n  id=\"\",\n  children\n}) => {\n  return (\n    <div className={className} id={id}>\n      <div className={`${styleVars.page.xl} grid-12 gutter-row-20 gutter-36-ns`}>\n        <div className={`${bodyClass}`}>\n          <h1 className={`${headingClass}`}>\n            {heading}\n          </h1>\n          {children}\n        </div>\n        {sideImg || sideComponent ? (\n          <div className={`${sideClass}`}>\n            {sideImg ? (\n              <Icon name={sideImg} className={sideImgClassName}/>\n            ) : (\n              sideComponent\n            )}\n          </div>\n        ): null}\n      </div>\n    </div>\n  );\n}\n\nexport default SectionBox;\n","import React, { useState } from \"react\";\nimport Layout from \"../../components/layout/index\";\nimport SEO from \"../../components/seo\";\nimport { SectionBox } from \"../../components/home\";\n\nconst handleScroll = (isModalOpen) => {\n  if (isModalOpen === true) {\n    document.documentElement.style.overflow = \"hidden\";\n  } else {\n    document.documentElement.style.overflowY = \"scroll\";\n  }\n};\n\nconst Page = (props) => {\n  const [isContactOpen, setContact] = useState(false);\n  const handleContact = () => {\n    // Look into state updates\n    // Look into state updates\n    handleScroll(!isContactOpen);\n    setContact(!isContactOpen);\n  };\n\n  return (\n    <Layout headerClass=\"\">\n      <SEO\n        title=\"A tiny HRM 27M beat Opus 4 and o3 on the ARC AGI benchmark\"\n        description={`That sounds very surprising. Most of the SOTA models we hear about are hundreds of billions to a few trillion parameters. HRM shows something different can work for reasoning on limited data.`}\n      />\n      <div\n        class=\"main-content\"\n        style={{\n          minHeight: \"100vh\",\n        }}\n      >\n        <SectionBox\n          heading=\"A tiny HRM 27M beat Opus 4 and o3 on the ARC AGI benchmark\"\n          headingClass=\"ma0 pa0 f2 f-headline-ns sig-blue fw-600\"\n          bodyClass=\"col-12 mw-100 center\"\n          className=\"pt16\"\n        />\n        <div className=\" pt0 pb5 pt10-ns pb20-ns\">\n          <div className=\"mw-l center\">\n            <p className=\"ma0 pa0 pl5 pr5 mt4 f4 f3-ns sig-grey\">\n              That sounds very surprising.\n              <br />\n              <br />\n              Most of the SOTA models we hear about (Claude, GPT, Gemini, Grok) are hundreds of billions to a few trillion parameters. We can't realistically run those locally. So when Sapient Intelligence published HRM (paper in June, model out in July) everyone was impressed that something a thousand times smaller can outperform larger counterparts on a ARC AGI benchmark. But size alone doesn't tell the full story.\n              <br />\n              <br />\n              HRM scored 32% on ARC AGI 1 and 2% on ARC AGI 2 without pre-training. That's the interesting part, as the current foundation models are trained on almost the entire internet. It shows something different can work for reasoning on limited data.\n              <br />\n              <br />\n              For context from recent large models:\n              <br />\n              - Qwen3-coder (480B parameter model) was trained on 7.5 trillion tokens\n              <br />\n              - Kimi-k2 (1T parameter model) saw 15.5 trillion tokens\n              <br />\n              <br />\n              As we understand HRM is not a foundation model. It lacks:\n              <br />\n              1. Broad pre-training data.\n              <br />\n              2. Transfer learning capability.\n              <br />\n              <br />\n              So we can't expect HRM to write polished emails or build software apps from scratch. What it can do is solve logical puzzles and show reasoning aptitude exactly what ARC AGI measures.\n              <br />\n              <br />\n              <strong>How does HRM work?</strong>\n              <br />\n              <br />\n              HRM's core novelty is architectural, inspired (loosely) by neuroscience. Instead of one transformer predicting the next token, HRM uses a dual recurrent loop. Two recurrent neural networks operating at different time scales:\n              <br />\n              <br />\n              - A fast, lower-level module that generates bursts of thinking (rapid, local processing)\n              <br />\n              - A slower, higher-level module that is more abstract and refines the fast module's outputs\n              <br />\n              <br />\n              Think of it as fast intuition + slow deliberation. The higher module provides guidance, the lower module produces quick hypotheses which are iteratively refined. This outer-refinement loop feeding fast bursts into a slower, abstract controller is what gives HRM its compositional reasoning advantage on specific tasks.\n              <br />\n              <br />\n              Transformers, by contrast, optimize next-token prediction over large contexts and scale extremely well in parallel training. RNNs historically struggled with long-term dependencies and early convergence to suboptimal representations. HRM sidesteps some of those issues by bifurcating timescales and adding that iterative refinement.\n              <br />\n              <br />\n              <br />\n              <strong>Why this matters?</strong>\n              <br />\n              <br />\n              HRM is not a transformer killer or an immediate replacement for foundation models. It's not a threat to the scale and pre-training paradigm that powers GPT-class models. Instead, it's a refinement in the AGI/reasoning direction an architecture that shows RNN-style systems can be competitive on reasoning tasks without trillions of tokens.\n              <br />\n              <br />\n              Now RNNs could come back. We've traded a lot of learning quality for speed of scaling. If new architectures learn better, we might not need monstrous models to get strong reasoning.\n              <br />\n              <br />\n              A path toward more efficient AGI-style reasoning. Transformer scale has limits eventual saturation, huge compute, and the impossibility of local runs. HRM hints at designs that might be more compute-friendly for certain classes of reasoning.\n              <br />\n              <br />\n              Biology-inspired loops are promising again. The outer refinement loop is simple but powerful iterate, correct, repeat. That's central to human reasoning and now seems useful in neural nets too.\n              <br />\n              <br />\n              Pre-training is still a massive advantage. Transformers win because they parallelize well and learn from massive batches. HRM hasn't been scaled with that same pre-training playbook yet.\n              <br />\n              <br />\n            </p>\n          </div>\n        </div>\n      </div>\n    </Layout>\n  );\n};\n\nexport default Page;"],"names":["_ref","heading","headingClass","bodyClass","sideComponent","sideImg","sideClass","className","sideImgClassName","id","children","React","styleVars","page","xl","Icon","name","props","isContactOpen","setContact","useState","Layout","headerClass","SEO","title","description","class","style","minHeight","SectionBox"],"sourceRoot":""}